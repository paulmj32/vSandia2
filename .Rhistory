summary(posterior.strong)
############################################################################################
### PRIORS
############################################################################################
# Default priors ... do these make sense? (Default are week priors)
prior_summary(bayes)
############################################################################################
### CREDIBLE INTERVALS
############################################################################################
posterior_interval(bayes)
?posterior_interval
# Confidence Intervals (95%)
conf_interval = predict(linear, newdata=data.frame(speed = cars$speed), interval="confidence", level = .95 )
# Confidence Intervals (95%)
conf_interval = predict(linear, newdata=data.frame(speed = cars$speed), interval="confidence", level = .95 )
# Confidence Intervals (95%)
conf_interval = predict(linear, newdata=data.frame(speed = cars$speed), interval="confidence", level = .95 )
df.gg1 = data.frame(speed = cars$speed,
dist = cars$dist,
yhat = linear$fitted.values,
CI.lower = conf_interval[,2],
CI.upper = conf_interval[,3])
ggplot(data = df.gg1) +
geom_point(aes(x = speed, y = dist)) +
geom_line(aes(x = speed, y = yhat), col = "red") +
geom_ribbon(aes(x = speed, ymin = CI.lower, ymax = CI.upper), fill = "red", alpha = 0.5)
?posterior_interval
############################################################################################
### CREDIBLE INTERVALS
############################################################################################
posterior_interval(bayes, prob = 0.95)
# Confidence Intervals (95%)
conf = confint(linear)
print(conf)
############################################################################################
### CREDIBLE INTERVALS
############################################################################################
posterior_interval(bayes, prob = 0.95)
# Credible Intervals (95%)
posterior_interval(bayes, prob = 0.95)
############################################################################################
### CREDIBLE INTERVALS
############################################################################################
# Confidence Intervals (95%)
# Question: What does this 95% confidence interval mean?
confint(linear)
# Credible Intervals (95%)
# Question: What does this 95% credible interval mean?
posterior_interval(bayes, prob = 0.95)
?quantile
quantile(bayes$speed, probs = c(0.025, 0.975))
library(rstanarm)
library(rstan)
library(tidyverse)
############################################################################################
### LINEAR MODEL
############################################################################################
# Data
data(cars)
head(cars)
plot(cars$speed, cars$dist)
cor(cars$speed, cars$dist)
# Linear regression
linear = lm(dist ~ speed, data = cars)
summary(linear)
lines(cars$speed, linear$fitted.values, col = "red")
#rsq = cor(glm_lin$fitted.values, cars$dist)^2
#rsq = 1 - sum((cars$dist - glm_lin$fitted.values)^2) / sum((cars$dist - mean(cars$dist))^2)
#r = rsq^(.5)
# conf_interval = predict(linear, newdata=data.frame(speed = cars$speed), interval="confidence", level = .95)
#
# df.gg1 = data.frame(speed = cars$speed,
#                     dist = cars$dist,
#                     yhat = linear$fitted.values,
#                     CI.lower = conf_interval[,2],
#                     CI.upper = conf_interval[,3])
# ggplot(data = df.gg1) +
#   geom_point(aes(x = speed, y = dist)) +
#   geom_line(aes(x = speed, y = yhat), col = "red") +
#   geom_ribbon(aes(x = speed, ymin = CI.lower, ymax = CI.upper), fill = "red", alpha = 0.5)
############################################################################################
### BAYESIAN MODEL
############################################################################################
# Bayesian linear model
bayes = stan_glm(dist ~ speed, data = cars, family = "gaussian")
summary(bayes)
# Diagnostics
stan_trace(bayes, pars=c("(Intercept)","speed","sigma"))
mean(cars$dist) #matches mean_PPD
# Posterior
posterior = as.data.frame(bayes)
summary(posterior)
############################################################################################
### PRIORS
############################################################################################
# Default priors ... do these make sense? (Default are week priors)
prior_summary(bayes)
# New priors
bayes.strong = stan_glm(dist ~ speed, data = cars, family = "gaussian",
prior_intercept = normal(-15, 7, autoscale = F),
prior = normal(2, 0.5, autoscale = F),
prior_aux = exponential(1/.05, autoscale = F)
)
posterior.strong = as.data.frame(bayes.strong)
summary(posterior.strong)
############################################################################################
### CREDIBLE INTERVALS
############################################################################################
# Confidence Intervals (95%)
# Question: How is this calculated, and what does this 95% confidence interval mean?
confint(linear)
# Credible Intervals (95%)
# Question: How is this calculated, and what does this 95% credible interval mean?
posterior_interval(bayes, prob = 0.95)
quantile(bayes$speed, probs = c(0.025, 0.975))
# Question: How would you get prediction intervals?
quantile(posterior$speed, probs = c(0.025, 0.975))
###############################################################################
### MY FIRST (AND LAST) BAYESIAN MODEL
###############################################################################
# Data
friends = 7
n = 10
# Prior
set.seed(32)
prior = runif(1000, 0, 1)
hist(prior)
# Likelihood (Generating Function)
likelihood = rbinom(1000, n, prior)
# Get Posterior (i.e., update prior to reflect data)
posterior = prior[which(likelihood == friends)]
hist(posterior)
# Let's try to get a cleaner approximation of the posterior and find out?
samples = 10000
set.seed(32)
prior = runif(samples, 0, 1)
likelihood = rbinom(samples, n, prior)
posterior = prior[which(likelihood == friends)]
hist(posterior)
###############################################################################
### TRYING DIFFERENT PRIORS (informative prior)
###############################################################################
# informative prior: you're not as cool as you think you are ... don't have many friends
set.seed(32)
prior.beta = rbeta(samples, 2, 5)
hist(prior.beta)
#likelihood
likelihood.beta = rbinom(samples, n, prior.beta)
# get posterior
posterior.beta = prior.beta[which(likelihood.beta == friends)]
hist(posterior.beta)
## Q3: What's different about the two posteriors?
## A3: The second is shifted toward the informative prior
summary(posterior)
summary(posterior.beta)
###############################################################################
### MORE DATA --> Through another party
###############################################################################
friends2 = 8
prior2 = sample(posterior.beta, size = samples, replace = T)
likelihood2 = rbinom(samples, n, prior2)
posterior2 = prior2[which(likelihood2 == friends2)]
hist(posterior2)
###############################################################################
### OH YEAH, WHAT ABOUT THAT NORMALZING?
###############################################################################
#https://mq-software-carpentry.github.io/statistics-with-r/06-bayesian-statistics/index.html
barplot(table(cut(posterior2, seq(0, 1, 0.05))) / 1, col = "skyblue")
barplot(table(cut(posterior2, seq(0, 1, 0.05))) / length(posterior2), col = "salmon")
###############################################################################
### SOME DOWNSIDES TO THIS APPROACH
###############################################################################
friends3 = 70
n3 = 100
set.seed(32)
prior.beta3 = rbeta(samples, 2, 5)
likelihood.beta3 = rbinom(samples, n3, prior.beta3)
posterior.beta3 = prior.beta3[which(likelihood.beta3 == friends3)]
hist(posterior.beta3)
## Uh oh ...
friends4 = 70
n4 = 100
samples4 = 1000000
samples4 = 1000000
set.seed(32)
prior.beta4 = rbeta(samples4, 2, 5)
likelihood.beta4 = rbinom(samples4, n4, prior.beta4)
posterior.beta4 = prior.beta4[which(likelihood.beta4 == friends4)]
hist(posterior.beta4)
library(rstanarm)
library(rstan)
library(tidyverse)
############################################################################################
### LINEAR MODEL
############################################################################################
# Data
data(cars)
head(cars)
plot(cars$speed, cars$dist)
cor(cars$speed, cars$dist)
# Linear regression
linear = lm(dist ~ speed, data = cars)
summary(linear)
lines(cars$speed, linear$fitted.values, col = "red")
############################################################################################
### BAYESIAN MODEL
############################################################################################
# Bayesian linear model
bayes = stan_glm(dist ~ speed, data = cars, family = "gaussian")
summary(bayes)
############################################################################################
### PRIORS
############################################################################################
# Default priors ... do these make sense? (Default are week priors)
prior_summary(bayes)
summary(posterior)
library(rstanarm)
library(rstan)
library(tidyverse)
############################################################################################
### LINEAR MODEL
############################################################################################
# Data
data(cars)
head(cars)
plot(cars$speed, cars$dist)
cor(cars$speed, cars$dist)
# Linear regression
linear = lm(dist ~ speed, data = cars)
summary(linear)
lines(cars$speed, linear$fitted.values, col = "red")
############################################################################################
### BAYESIAN MODEL
############################################################################################
# Bayesian linear model
bayes = stan_glm(dist ~ speed, data = cars, family = "gaussian")
summary(bayes)
# Diagnostics
stan_trace(bayes, pars=c("(Intercept)","speed","sigma"))
mean(cars$dist) #matches mean_PPD
# Posterior
posterior = as.data.frame(bayes)
summary(posterior)
stan_hist(bayes, pars=c("speed"), bins=40)
stan_hist(bayes, pars=c("(Intercept)","speed","sigma"))
stan_hist(bayes, pars=c("(Intercept)","speed","sigma"), bins=40)
stan_hist(bayes, pars=c("(Intercept)","speed","sigma"), bins=30)
stan_hist(bayes, pars=c("(Intercept)","speed","sigma"))
############################################################################################
### LET's TALK ABOUT PRIORS
############################################################################################
# Default priors ... do these make sense? (Default are week priors)
prior_summary(bayes)
# Credible interval
post_samps_speed <- as.data.frame(bayes, pars=c("speed"))
# Credible interval
post_samps = as.data.frame(bayes, pars=c("(Intercept)","speed","sigma"))
mean_speed = mean(post_samps$speed)
ci_speed <- quantile(post_samps$speed, probs=c(0.025, 0.975)) # posterior 95% interval
############################################################################################
### LET's TALK ABOUT PRIORS
############################################################################################
# Default priors ... do these make sense? (Default are week priors)
prior_summary(bayes)
sd(cars$dist)
# exponential: mean ~ 1/lambda ... 1/1 --> mean standard deviation
sd(cars$dist)
# exponential: mean ~ 1/lambda ... 1/1 --> mean standard deviation
1/0.039
sd(cars$dist)
options(java.parameters = "-Xmx10g")
library(bartMachine)
library(tidyverse)
library(tidymodels)
library(tidycensus)
library(sf)
library(xgboost)
library(parallel)
library(doParallel)
library(vip)
library(spdep)
library(pdp)
# library(drat) # these are used to install hurricaneexposure
# addRepo("geanders")
# install.packages("hurricaneexposuredata")
library(hurricaneexposuredata)
library(hurricaneexposure)
library(spatialreg)
library(gstat)
library(ggpubr)
library(grid)
library(gridExtra)
library(cowplot)
################################################################################
#### PRE-PROCESSING ############################################################
################################################################################
#sort(sapply(ls(),function(x){object.size(get(x))}), decreasing = T)
# Parallel processing setup
num_cores = detectCores() - 1
unregister_dopar = function() { #function to un-register parallel processing in doParallel
env <- foreach:::.foreachGlobals
rm(list=ls(name=env), pos=env)
}
set_bart_machine_num_cores(num_cores = num_cores)
our_events = c(
"droughts",
"extreme_cold",
"extreme_heat",
"floods",
#"hail", #no events
"high_winds",
"hurricanes",
#"tornadoes", #no events
"wildfires",
"winter_storms"
)
# Load data and select final DVs
load(file = "Data/processed/sf_data_ALL_nototal.Rda")
# Load data and select final DVs
setwd("/Users/paul/vSandia2")
load(file = "Data/processed/sf_data_ALL_nototal.Rda")
sf_data = sf_data_ALL %>%
dplyr::filter(hurricanes >= 1) %>% #filter to event of interest
mutate(ln_hrs = log(duration_hr)) %>%
mutate(ln_cust = log(max_cust_out)) %>%
#mutate(pct_cust = max_frac_cust_out) %>%
mutate(pct_cust = log(max_frac_cust_out)) %>%
dplyr::select(-c(POPULATION, mean_cust_out, mean_frac_cust_out, max_cust_out, max_frac_cust_out,
duration_hr, all_of(our_events), avalanche)) %>%
relocate(c(ln_hrs, ln_cust, pct_cust))
rm(list=c("sf_data_ALL"))
gc()
df_data = sf_data %>%
st_set_geometry(NULL)
df_hiba = initial_split(df_data, prop = 0.4, strata = "ln_hrs")
df_hiba2 = training(df_hiba)
class(df_hiba2)
df_hurricanes = df_hiba2 %>%
dplyr::select(-ln_cust, -pct_cust, -GEOID, -delta_vapor, -delta_T10M, -delta_WIND10M, -spi06, -spi24)
View(df_hiba2)
df_hurricanes = df_hiba2 %>%
dplyr::select(-ln_cust, -pct_cust, -GEOID, -delta_vapor, -delta_T10M, -delta_WIND10M, -spi06, -spi24, -ARCHENG:PROXMET)
df_hurricanes = df_hiba2 %>%
dplyr::select(-ln_cust, -pct_cust, -GEOID, -delta_vapor, -delta_T10M, -delta_WIND10M, -spi06, -spi24, -c(ARCHENG:PROXMET))
df_hurricanes = df_hiba2 %>%
dplyr::select(ln_hrs, max_WIND10M:max_T10M, delta_pressure, spi03, spi12, Barren:RZ_mode)
df_hurricanes = df_hiba2 %>%
dplyr::select(ln_hrs, max_WIND10M:max_T10M, delta_pressure, spi03, spi12, Barren:RZ_mode
,QPOVTY, QMINORITY, QFEMALE, QED12, QSSBEN
)
save(df_hurricanes, file = "Data/df_hurricanes.Rda")
load("/Users/paul/Desktop/Hiba Lecture/df_hurricanes.Rda")
library(tidyverse)
load("/Users/paul/Desktop/Hiba Lecture/df_hurricanes.Rda")
#natural log of hurricane outage duration (county x event)
hist(df_hurricanes$ln_hrs)
head(df_hurricanes)
install.packages("dbarts")
library(dbarts)
summary(df_hurricanes)
library(tidymodels)
?bart
# Train/Test
set.seed(23)
df_split = initial_split(df_hurricanes, prop = 0.80, strata = "ln_hrs")
df_train = training(df_split)
df_test = test(df_split)
df_test = testing(df_split)
x.train = df_train %>% dplyr::select(-ln_hrs)
y.train = df_train$ln_hrs
x.test = df_test %>% dplyr::select(-ln_hrs)
# Use BART to predict outages
bart1 = dbarts::bart(x.train = x.train, y.train = y.train, x.test = x.test)
plot(bart1)
plot(
bart1,
plquants = c(0.05, 0.95), cols = c('blue', 'black'))
library(varimp)
library(vip)
varimp = vip(bart1)
library(bartm)
library(bartMachine
library(bartMachine)
library(bartMachine)
# Use BART to predict outages
bart = bartMachine(X = x.train, y = y.train)
check_bart_error_assumptions()
check_bart_error_assumptions(bart)
library(tidyverse)
library(tidymodels)
library(bartMachine)
load("/Users/paul/Desktop/Hiba Lecture/df_hurricanes.Rda")
head(df_hurricanes)
summary(df_hurricanes)
# Train/Test
set.seed(23)
df_split = initial_split(df_hurricanes, prop = 0.80, strata = "ln_hrs")
df_train = training(df_split)
df_test = testing(df_split)
x.train = df_train %>% dplyr::select(-ln_hrs)
y.train = df_train$ln_hrs
x.test = df_test %>% dplyr::select(-ln_hrs)
# Use BART to predict outages
bart = bartMachine(X = x.train, y = y.train)
# Check model assumptions
check_bart_error_assumptions(bart)
plot_convergence_diagnostics(bart)
bart
summary(bart)
plot_y_vs_yhat(bart_machine_cv, credible_intervals = TRUE)
plot_y_vs_yhat(bart, credible_intervals = TRUE)
plot_y_vs_yhat(bart, prediction_intervals = TRUE)
fit = predict(bart, x.train)
CI = roun(calc_credible_intervals(bart, x.train, ci_conf = 0.95), 2)
CI = round(calc_credible_intervals(bart, x.train, ci_conf = 0.95), 2)
gg = data.frame(mean = fit,
lower = CI[,1],
upper = CI[,2],
actual = x.test)
gg = data.frame(mean = fit,
lower = CI[,1],
upper = CI[,2],
actual = y.train)
arrange(gg, fit)
arrange(gg, actual)
gg = data.frame(mean = fit,
lower = CI[,1],
upper = CI[,2],
actual = y.train,
index = seq.int(nrow(x.train)))
View(gg)
# ggplot
fit = predict(bart, x.train)
CI = round(calc_credible_intervals(bart, x.train, ci_conf = 0.95), 2)
gg = data.frame(mean = fit,
lower = CI[,1],
upper = CI[,2],
actual = y.train
)
arrange(gg, actual)
gg$index = seq.int(nrow(gg))
gg = data.frame(fit = fit,
lower = CI[,1],
upper = CI[,2],
actual = y.train
)
arrange(gg, actual)
gg$index = seq.int(nrow(gg))
gg = data.frame(mean = fit,
lower = CI[,1],
upper = CI[,2],
actual = y.train
)
arrange(gg, actual)
gg$index = seq.int(nrow(gg))
ggplot(data = gg) +
geom_point(aes(y = actual, x = index), col = "gray30") +
geom_line(aes(y = mean, x = index), col = "red") +
geom_ribbon(aes(x = index, ymin = lower, ymax = upper), fill = "red", alpha = 0.5)
# Test predictions
fit = predict(bart, x.test)
CI = round(calc_credible_intervals(bart, x.test, ci_conf = 0.95), 2)
gg = data.frame(mean = fit,
lower = CI[,1],
upper = CI[,2],
actual = y.test
)
y.test = df_test$ln_hrs
gg = data.frame(mean = fit,
lower = CI[,1],
upper = CI[,2],
actual = y.test
)
arrange(gg, actual)
gg$index = seq.int(nrow(gg))
ggplot(data = gg) +
geom_point(aes(y = actual, x = index), col = "gray30") +
geom_line(aes(y = mean, x = index), col = "red") +
geom_ribbon(aes(x = index, ymin = lower, ymax = upper), fill = "red", alpha = 0.5)
# Variable Importance
vimp = investigate_var_importance(bart)
# Test predictions
predictions = predict(bart, x.test)
CI = round(calc_credible_intervals(bart, x.test, ci_conf = 0.95), 2)
gg = data.frame(mean = predictions,
lower = CI[,1],
upper = CI[,2],
actual = y.test
)
arrange(gg, actual)
gg$index = seq.int(nrow(gg))
ggplot(data = gg) +
geom_point(aes(y = actual, x = index), col = "gray30") +
geom_line(aes(y = mean, x = index), col = "red") +
geom_ribbon(aes(x = index, ymin = lower, ymax = upper), fill = "red", alpha = 0.5)
ggplot(data = gg) +
geom_point(aes(y = actual, x = index), col = "gray30") +
geom_line(aes(y = mean, x = index), col = "red") +
geom_ribbon(aes(x = index, ymin = lower, ymax = upper), fill = "red", alpha = 0.5) +
ylab("Outage Duration (ln-hrs)") +
xlab("Outage Event") +
ggtitle("Predicting Hurricane Outages")
ggplot(data = gg) +
geom_point(aes(y = actual, x = index), col = "gray30") +
geom_line(aes(y = mean, x = index), col = "red") +
geom_ribbon(aes(x = index, ymin = lower, ymax = upper), fill = "red", alpha = 0.5) +
ylab("Outage Duration (ln-hrs)") +
xlab("Outage Event") +
ggtitle("Predicting Hurricane Outages") +
theme(plot.title = element_text(hjust = 0.5))
rsq_bart = format(round(1 - sum((y.test - predictions)^2) / sum((y.test - mean(y.test))^2), 3), nsmall = 3)
rsq_bart = 1 - sum((y.test - predictions)^2) / sum((y.test - mean(y.test))^2)
