finalize_workflow(xgb_best) %>%
fit(df_data)
final_obj = extract_fit_parsnip(final_model)$fit
importance = xgb.importance(model = final_obj)
head(importance, n = 20)
vip(final_obj, n = 20)
bart_vimp = investigate_var_importance(bart_fit)
res_xgb = as.vector(xgb_predictions$.pred) - y_test
res_lre = as.vector(lre_predictions$.pred) - y_test
res = res_xgb
# Normality of residuals
# http://www.sthda.com/english/wiki/normality-test-in-r
qqnorm(res)
qqline(res)
shapiro.test(res)
# Heteroskedacity of residuals
fitted = as.vector(xgb_predictions$.pred)
plot(fitted, res_xgb)
bart_assmp = check_bart_error_assumptions(bart_fit)
# Spatial dependency of residuals
# Base county map
load(file = "Data/processed/county_map_proj.Rda")
rr = data.frame(as.factor(df_test$GEOID))
rr$res = res
colnames(rr) = c("GEOID", "resid")
plot(rr) # looks good but use varigoram below to confirm
county_centroid = st_centroid(county_map_proj) # get center of counties
county_lonlat = county_centroid %>%
mutate(X = unlist(map(county_centroid$geometry,1)),
Y = unlist(map(county_centroid$geometry,2))) %>%
dplyr::select(-NAME, -POPULATION) %>%
inner_join(rr, by = c("GEOID")) %>%
rename(Z = resid)
county_lonlat_sp = as_Spatial(county_lonlat)
vgram = variogram(Z~1, county_lonlat_sp)
plot(vgram)
options(java.parameters = "-Xmx10g")
library(bartMachine)
library(tidyverse)
library(tidymodels)
library(tidycensus)
library(sf)
library(xgboost)
library(parallel)
library(doParallel)
library(vip)
library(spdep)
library(pdp)
# library(drat) # these are used to install hurricaneexposure
# addRepo("geanders")
# install.packages("hurricaneexposuredata")
library(hurricaneexposuredata)
library(hurricaneexposure)
library(spatialreg)
library(gstat)
library(ggpubr)
################################################################################
#### PRE-PROCESSING ############################################################
################################################################################
#sort(sapply(ls(),function(x){object.size(get(x))}), decreasing = T)
# Parallel processing setup
num_cores = detectCores() - 1
unregister_dopar = function() { #function to un-register parallel processing in doParallel
env <- foreach:::.foreachGlobals
rm(list=ls(name=env), pos=env)
}
set_bart_machine_num_cores(num_cores = num_cores)
our_events = c(
"droughts",
"extreme_cold",
"extreme_heat",
"floods",
#"hail", #no events
"high_winds",
"hurricanes",
#"tornadoes", #no events
"wildfires",
"winter_storms"
)
# Load data and select final DVs
load(file = "Data/processed/sf_data_ALL_nototal.Rda")
sf_data = sf_data_ALL %>%
dplyr::filter(floods >= 1) %>% #filter to event of interest
mutate(ln_hrs = log(duration_hr)) %>%
mutate(ln_cust = log(max_cust_out)) %>%
#mutate(pct_cust = max_frac_cust_out) %>%
mutate(pct_cust = log(max_frac_cust_out)) %>%
dplyr::select(-c(POPULATION, mean_cust_out, mean_frac_cust_out, max_cust_out, max_frac_cust_out,
duration_hr, all_of(our_events), avalanche)) %>%
relocate(c(ln_hrs, ln_cust, pct_cust))
rm(list=c("sf_data_ALL"))
gc()
df_data = sf_data %>%
st_set_geometry(NULL)
# Split into training vs testing
set.seed(23)
df_split = initial_split(df_data, prop = 0.80, strata = "ln_hrs")
df_train = training(df_split)
df_test = testing(df_split)
df_cv = vfold_cv(df_train, v = 10, repeats = 1)
df_preds = df_data %>% dplyr::select(-c(ln_hrs, ln_cust, pct_cust, GEOID))
# Recipes for tidymodels
recipe_hrs = recipe(ln_hrs ~ . , data = df_data) %>% step_rm(ln_cust, pct_cust, GEOID) %>% step_naomit(ln_hrs)
recipe_pct = recipe(pct_cust ~ . , data = df_data) %>% step_rm(ln_hrs, ln_cust, GEOID) %>% step_naomit(pct_cust)
################################################################################
#### MACHINE LEARNING ##########################################################
################################################################################
### Define which recipe and responses you want to use
recipe_mine = recipe_hrs
## Recipe - ln_hrs
y_train = df_train %>% pull(ln_hrs) %>% na.omit()
y_test = df_test %>% pull(ln_hrs) %>% na.omit()
X_train = df_train %>% dplyr::select(-c(GEOID, ln_hrs, ln_cust, pct_cust)) %>% na.omit()
X_test = df_test %>% dplyr::select(-c(GEOID, ln_hrs, ln_cust, pct_cust)) %>% na.omit()
## Recipe - pct_cust
# y_train = df_train %>% na.omit() %>% pull(pct_cust)
# y_test = df_test %>% na.omit() %>% pull(pct_cust)
# X_train = df_train %>% na.omit() %>% dplyr::select(-c(GEOID, ln_hrs, ln_cust, pct_cust))
# X_test = df_test %>% na.omit() %>% dplyr::select(-c(GEOID, ln_hrs, ln_cust, pct_cust))
### Lasso, Ridge Regression, and Elastic Net ###################################
#https://www.tidyverse.org/blog/2020/11/tune-parallel/
show_model_info("linear_reg")
lre_model = linear_reg(penalty = tune(), mixture = tune()) %>% #lambda (penalty) and alpha/mixture (1 lasso, 0 ridge)
set_engine("glmnet") %>%
translate()
lre_work = workflow() %>%
add_recipe(recipe_mine) %>%
add_model(lre_model)
#lre_grid = dials::grid_regular(parameters(penalty(), mixture()), levels = c(5, 5))
set.seed(32); lre_grid = dials::grid_max_entropy(parameters(penalty(), mixture()), size = 40)
cl = makeCluster(num_cores, type = "FORK")
registerDoParallel(cl, cores = num_cores)
lre_tune = lre_work %>%
tune_grid(resamples = df_cv,
grid = lre_grid,
metrics = metric_set(yardstick::rmse, yardstick::rsq),
control = tune::control_grid(verbose = T, allow_par = T, parallel_over = "resamples")
) #parallel processing turns off verbose
stopCluster(cl)
unregister_dopar()
show_best(lre_tune, metric = "rmse")
lre_tune_results = lre_tune %>% collect_metrics()
lre_best = lre_tune %>% select_best(metric = "rmse")
lre_fit = lre_work %>%
finalize_workflow(lre_best) %>%
last_fit(df_split)
lre_test = lre_fit %>% collect_metrics() #metrics evaluated on test sample (b/c last_fit() function)
lre_predictions = lre_fit %>% collect_predictions() #predictions for test sample (b/c last_fit() function)
rsq_lre = paste(lre_test %>% dplyr::filter(.metric == "rsq") %>% pull(.estimate) %>% round(3) %>% format(nsmall = 3))
cverror_lre = paste(show_best(lre_tune, metric = "rmse") %>% dplyr::slice(1) %>% pull(mean) %>% round(3) %>% format(nsmall = 3))
show_model_info("boost_tree")
xgb_model = boost_tree(mode = "regression", trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(), loss_reduction = tune(), mtry = tune()) %>%
set_engine(engine = "xgboost") %>%
translate()
xgb_work = workflow() %>%
add_recipe(recipe_mine) %>%
add_model(xgb_model)
set.seed(32); xgb_grid = dials::grid_max_entropy(parameters(trees(), min_n(), tree_depth(), learn_rate(), loss_reduction(), finalize(mtry(), df_preds)), size = 100)
cl = makeCluster(num_cores, type = "FORK")
registerDoParallel(cl, cores = num_cores)
xgb_tune = xgb_work %>%
tune_grid(resamples = df_cv,
grid = xgb_grid,
metrics = metric_set(yardstick::rmse, yardstick::rsq),
control = tune::control_grid(verbose = T, allow_par = T, parallel_over = "resamples")
) #parallel processing turns off verbose
stopCluster(cl)
unregister_dopar()
show_best(xgb_tune, metric = "rmse")
xgb_tune_results = xgb_tune %>% collect_metrics()
xgb_best = xgb_tune %>% select_best(metric = "rmse")
xgb_fit = xgb_work %>%
finalize_workflow(xgb_best) %>%
last_fit(df_split)
xgb_test = xgb_fit %>% collect_metrics() #metrics evaluated on test sample (b/c last_fit() function)
xgb_predictions = xgb_fit %>% collect_predictions() #predictions for test sample (b/c last_fit() function)
rsq_xgb = paste(xgb_test %>% dplyr::filter(.metric == "rsq") %>% pull(.estimate) %>% round(3) %>% format(nsmall = 3))
cverror_xgb = paste(show_best(xgb_tune, metric = "rmse") %>% dplyr::slice(1) %>% pull(mean) %>% round(3) %>% format(nsmall = 3))
### BART #######################################################################
bart_fit = bartMachineCV(data.frame(X_train), y_train, k_folds = 10, serialize = T)
rsq_bart = format(round(1 - sum((y_test - bart_predictions)^2) / sum((y_test - mean(y_test))^2), 3), nsmall = 3)
bart_predictions = predict(bart_fit, data.frame(X_test))
rsq_bart = format(round(1 - sum((y_test - bart_predictions)^2) / sum((y_test - mean(y_test))^2), 3), nsmall = 3)
bart_cv = k_fold_cv(data.frame(X_train), y_train, k_folds = 10)
cverror_bart = format(round(bart_cv$rmse, 3), nsmall = 3)
gg = dplyr::tibble(actual = y_test,
eNet = as.vector(lre_predictions$.pred),
bart = as.vector(bart_predictions),
#rf = as.vector(rf_predictions$.pred),
xgb = as.vector(xgb_predictions$.pred)
)
gg = arrange(gg, actual)
gg$index = seq.int(nrow(gg))
gg_actual = gg %>% dplyr::select(index, actual)
gg_pred = gg %>% dplyr::select(-actual) %>% pivot_longer(!index, names_to = "Model", values_to = "ypred")
gg_all = gg %>% pivot_longer(!index, names_to = "Model", values_to = "ypred")
color_vec = c("black", "#FDE725FF", "#35B779FF", "#440154FF")
lty_vec = c(1, 1, 1, 1)
alpha_vec = c(1, 0.6, 0.6, 0.6)
plot_filtering_estimates2 <- function(df) {
p = ggplot() +
theme_classic() +
geom_hline(yintercept = mean(gg$actual, na.rm = T), linetype="dashed", color = "gray50", alpha = 0.85) +
geom_line(data = gg_all, aes(x = index, y = ypred, color = Model, lty = Model, alpha = Model)) +
scale_color_manual(
values = color_vec,
labels = c("Actual",
bquote("BART (" * R^2 ~ "=" ~ .(rsq_bart) * "," ~ RMSE[cv] ~ "=" ~ .(cverror_bart) * ")"),
bquote("eNET (" * R^2 ~ "=" ~ .(rsq_lre) * "," ~ RMSE[cv] ~ "=" ~ .(cverror_lre) * ")"),
bquote("XGB (" * R^2 ~ "=" ~ .(rsq_xgb) * "," ~ RMSE[cv] ~ "=" ~ .(cverror_xgb) * ")")
),
name = element_blank()) +
scale_linetype_manual(
values = lty_vec,
labels = c("Actual",
bquote("BART (" * R^2 ~ "=" ~ .(rsq_bart) * "," ~ RMSE[cv] ~ "=" ~ .(cverror_bart) * ")"),
bquote("eNET (" * R^2 ~ "=" ~ .(rsq_lre) * "," ~ RMSE[cv] ~ "=" ~ .(cverror_lre) * ")"),
bquote("XGB (" * R^2 ~ "=" ~ .(rsq_xgb) * "," ~ RMSE[cv] ~ "=" ~ .(cverror_xgb) * ")")
),
name = element_blank()) +
scale_alpha_manual(
values = alpha_vec,
labels = c("Actual",
bquote("BART (" * R^2 ~ "=" ~ .(rsq_bart) * "," ~ RMSE[cv] ~ "=" ~ .(cverror_bart) * ")"),
bquote("eNET (" * R^2 ~ "=" ~ .(rsq_lre) * "," ~ RMSE[cv] ~ "=" ~ .(cverror_lre) * ")"),
bquote("XGB (" * R^2 ~ "=" ~ .(rsq_xgb) * "," ~ RMSE[cv] ~ "=" ~ .(cverror_xgb) * ")")
),
name = element_blank()) +
scale_y_continuous(labels = function(x) paste0(x)) +
xlab("Index (County x Event)") +
ylab("Hours (ln)") +
ggtitle("Flood Outage Duration: Test Sample") +
#ylab("Max Customers Out (ln - %)") +
#ggtitle("Hurricane Outages: Test Sample") +
# guides(
#   color = guide_legend(order = 2),
#   shape = guide_legend(order = 1),
#   linetype = guide_legend(order = 2)
# ) +
theme(legend.spacing.y = unit(-0.25, "cm"),
legend.direction = "vertical",
legend.box = "vertical",
legend.position = c(.225, .8),
plot.title = element_text(hjust = 0.5)
)
print(p)
}
plot_filtering_estimates2(gg)
final_lre = lre_work %>%
finalize_workflow(lre_best) %>%
fit(df_data)
print(tidy(final_lre), n = 150)
final_model = xgb_work %>%
finalize_workflow(xgb_best) %>%
fit(df_data)
final_obj = extract_fit_parsnip(final_model)$fit
importance = xgb.importance(model = final_obj)
head(importance, n = 20)
vip(final_obj, n = 20)
bart_vimp = investigate_var_importance(bart_fit)
res_xgb = as.vector(xgb_predictions$.pred) - y_test
res_lre = as.vector(lre_predictions$.pred) - y_test
res = res_xgb
# Normality of residuals
# http://www.sthda.com/english/wiki/normality-test-in-r
qqnorm(res)
qqline(res)
shapiro.test(res)
# Heteroskedacity of residuals
fitted = as.vector(xgb_predictions$.pred)
plot(fitted, res_xgb)
bart_assmp = check_bart_error_assumptions(bart_fit)
load(file = "Data/processed/county_map_proj.Rda")
rr = data.frame(as.factor(df_test$GEOID))
rr$res = res
colnames(rr) = c("GEOID", "resid")
plot(rr) # looks good but use varigoram below to confirm
county_centroid = st_centroid(county_map_proj) # get center of counties
county_lonlat = county_centroid %>%
mutate(X = unlist(map(county_centroid$geometry,1)),
Y = unlist(map(county_centroid$geometry,2))) %>%
dplyr::select(-NAME, -POPULATION) %>%
inner_join(rr, by = c("GEOID")) %>%
rename(Z = resid)
county_lonlat_sp = as_Spatial(county_lonlat)
vgram = variogram(Z~1, county_lonlat_sp)
plot(vgram)
save.image("~/vSandia2/Data/DeepDive/flood_hrs.RData")
options(java.parameters = "-Xmx10g")
library(bartMachine)
library(tidyverse)
library(tidymodels)
library(tidycensus)
library(sf)
library(xgboost)
library(parallel)
library(doParallel)
library(vip)
library(spdep)
library(pdp)
# library(drat) # these are used to install hurricaneexposure
# addRepo("geanders")
# install.packages("hurricaneexposuredata")
library(hurricaneexposuredata)
library(hurricaneexposure)
library(spatialreg)
library(gstat)
library(ggpubr)
################################################################################
#### PRE-PROCESSING ############################################################
################################################################################
#sort(sapply(ls(),function(x){object.size(get(x))}), decreasing = T)
# Parallel processing setup
num_cores = detectCores() - 1
unregister_dopar = function() { #function to un-register parallel processing in doParallel
env <- foreach:::.foreachGlobals
rm(list=ls(name=env), pos=env)
}
set_bart_machine_num_cores(num_cores = num_cores)
our_events = c(
"droughts",
"extreme_cold",
"extreme_heat",
"floods",
#"hail", #no events
"high_winds",
"hurricanes",
#"tornadoes", #no events
"wildfires",
"winter_storms"
)
# Load data and select final DVs
load(file = "Data/processed/sf_data_ALL_nototal.Rda")
sf_data = sf_data_ALL %>%
dplyr::filter(high_winds >= 1) %>% #filter to event of interest
mutate(ln_hrs = log(duration_hr)) %>%
mutate(ln_cust = log(max_cust_out)) %>%
#mutate(pct_cust = max_frac_cust_out) %>%
mutate(pct_cust = log(max_frac_cust_out)) %>%
dplyr::select(-c(POPULATION, mean_cust_out, mean_frac_cust_out, max_cust_out, max_frac_cust_out,
duration_hr, all_of(our_events), avalanche)) %>%
relocate(c(ln_hrs, ln_cust, pct_cust))
rm(list=c("sf_data_ALL"))
gc()
df_data = sf_data %>%
st_set_geometry(NULL)
# Split into training vs testing
set.seed(23)
df_split = initial_split(df_data, prop = 0.80, strata = "ln_hrs")
df_train = training(df_split)
df_test = testing(df_split)
df_cv = vfold_cv(df_train, v = 10, repeats = 1)
df_preds = df_data %>% dplyr::select(-c(ln_hrs, ln_cust, pct_cust, GEOID))
# Recipes for tidymodels
recipe_hrs = recipe(ln_hrs ~ . , data = df_data) %>% step_rm(ln_cust, pct_cust, GEOID) %>% step_naomit(ln_hrs)
recipe_pct = recipe(pct_cust ~ . , data = df_data) %>% step_rm(ln_hrs, ln_cust, GEOID) %>% step_naomit(pct_cust)
################################################################################
#### MACHINE LEARNING ##########################################################
################################################################################
### Define which recipe and responses you want to use
recipe_mine = recipe_hrs
## Recipe - ln_hrs
y_train = df_train %>% pull(ln_hrs) %>% na.omit()
y_test = df_test %>% pull(ln_hrs) %>% na.omit()
X_train = df_train %>% dplyr::select(-c(GEOID, ln_hrs, ln_cust, pct_cust)) %>% na.omit()
X_test = df_test %>% dplyr::select(-c(GEOID, ln_hrs, ln_cust, pct_cust)) %>% na.omit()
## Recipe - pct_cust
# y_train = df_train %>% na.omit() %>% pull(pct_cust)
# y_test = df_test %>% na.omit() %>% pull(pct_cust)
# X_train = df_train %>% na.omit() %>% dplyr::select(-c(GEOID, ln_hrs, ln_cust, pct_cust))
# X_test = df_test %>% na.omit() %>% dplyr::select(-c(GEOID, ln_hrs, ln_cust, pct_cust))
### Lasso, Ridge Regression, and Elastic Net ###################################
#https://www.tidyverse.org/blog/2020/11/tune-parallel/
show_model_info("linear_reg")
lre_model = linear_reg(penalty = tune(), mixture = tune()) %>% #lambda (penalty) and alpha/mixture (1 lasso, 0 ridge)
set_engine("glmnet") %>%
translate()
lre_work = workflow() %>%
add_recipe(recipe_mine) %>%
add_model(lre_model)
#lre_grid = dials::grid_regular(parameters(penalty(), mixture()), levels = c(5, 5))
set.seed(32); lre_grid = dials::grid_max_entropy(parameters(penalty(), mixture()), size = 40)
cl = makeCluster(num_cores, type = "FORK")
registerDoParallel(cl, cores = num_cores)
lre_tune = lre_work %>%
tune_grid(resamples = df_cv,
grid = lre_grid,
metrics = metric_set(yardstick::rmse, yardstick::rsq),
control = tune::control_grid(verbose = T, allow_par = T, parallel_over = "resamples")
) #parallel processing turns off verbose
stopCluster(cl)
unregister_dopar()
show_best(lre_tune, metric = "rmse")
lre_tune_results = lre_tune %>% collect_metrics()
lre_best = lre_tune %>% select_best(metric = "rmse")
lre_fit = lre_work %>%
finalize_workflow(lre_best) %>%
last_fit(df_split)
lre_test = lre_fit %>% collect_metrics() #metrics evaluated on test sample (b/c last_fit() function)
lre_predictions = lre_fit %>% collect_predictions() #predictions for test sample (b/c last_fit() function)
rsq_lre = paste(lre_test %>% dplyr::filter(.metric == "rsq") %>% pull(.estimate) %>% round(3) %>% format(nsmall = 3))
cverror_lre = paste(show_best(lre_tune, metric = "rmse") %>% dplyr::slice(1) %>% pull(mean) %>% round(3) %>% format(nsmall = 3))
show_model_info("boost_tree")
xgb_model = boost_tree(mode = "regression", trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(), loss_reduction = tune(), mtry = tune()) %>%
set_engine(engine = "xgboost") %>%
translate()
xgb_work = workflow() %>%
add_recipe(recipe_mine) %>%
add_model(xgb_model)
set.seed(32); xgb_grid = dials::grid_max_entropy(parameters(trees(), min_n(), tree_depth(), learn_rate(), loss_reduction(), finalize(mtry(), df_preds)), size = 100)
cl = makeCluster(num_cores, type = "FORK")
registerDoParallel(cl, cores = num_cores)
xgb_tune = xgb_work %>%
tune_grid(resamples = df_cv,
grid = xgb_grid,
metrics = metric_set(yardstick::rmse, yardstick::rsq),
control = tune::control_grid(verbose = T, allow_par = T, parallel_over = "resamples")
) #parallel processing turns off verbose
stopCluster(cl)
unregister_dopar()
show_best(xgb_tune, metric = "rmse")
xgb_tune_results = xgb_tune %>% collect_metrics()
xgb_best = xgb_tune %>% select_best(metric = "rmse")
xgb_fit = xgb_work %>%
finalize_workflow(xgb_best) %>%
last_fit(df_split)
xgb_test = xgb_fit %>% collect_metrics() #metrics evaluated on test sample (b/c last_fit() function)
xgb_predictions = xgb_fit %>% collect_predictions() #predictions for test sample (b/c last_fit() function)
rsq_xgb = paste(xgb_test %>% dplyr::filter(.metric == "rsq") %>% pull(.estimate) %>% round(3) %>% format(nsmall = 3))
cverror_xgb = paste(show_best(xgb_tune, metric = "rmse") %>% dplyr::slice(1) %>% pull(mean) %>% round(3) %>% format(nsmall = 3))
bart_fit = bartMachineCV(data.frame(X_train), y_train, k_folds = 10, serialize = T)
bart_predictions = predict(bart_fit, data.frame(X_test))
#save(bart_fit, file = "bart_fit.Rda")
rsq_bart = format(round(1 - sum((y_test - bart_predictions)^2) / sum((y_test - mean(y_test))^2), 3), nsmall = 3)
bart_cv = k_fold_cv(data.frame(X_train), y_train, k_folds = 10)
cverror_bart = format(round(bart_cv$rmse, 3), nsmall = 3)
gg = dplyr::tibble(actual = y_test,
eNet = as.vector(lre_predictions$.pred),
bart = as.vector(bart_predictions),
#rf = as.vector(rf_predictions$.pred),
xgb = as.vector(xgb_predictions$.pred)
)
gg = arrange(gg, actual)
gg$index = seq.int(nrow(gg))
gg_actual = gg %>% dplyr::select(index, actual)
gg_pred = gg %>% dplyr::select(-actual) %>% pivot_longer(!index, names_to = "Model", values_to = "ypred")
gg_all = gg %>% pivot_longer(!index, names_to = "Model", values_to = "ypred")
color_vec = c("black", "#FDE725FF", "#35B779FF", "#440154FF")
lty_vec = c(1, 1, 1, 1)
alpha_vec = c(1, 0.6, 0.6, 0.6)
plot_filtering_estimates2 <- function(df) {
p = ggplot() +
theme_classic() +
geom_hline(yintercept = mean(gg$actual, na.rm = T), linetype="dashed", color = "gray50", alpha = 0.85) +
geom_line(data = gg_all, aes(x = index, y = ypred, color = Model, lty = Model, alpha = Model)) +
scale_color_manual(
values = color_vec,
labels = c("Actual",
bquote("BART (" * R^2 ~ "=" ~ .(rsq_bart) * "," ~ RMSE[cv] ~ "=" ~ .(cverror_bart) * ")"),
bquote("eNET (" * R^2 ~ "=" ~ .(rsq_lre) * "," ~ RMSE[cv] ~ "=" ~ .(cverror_lre) * ")"),
bquote("XGB (" * R^2 ~ "=" ~ .(rsq_xgb) * "," ~ RMSE[cv] ~ "=" ~ .(cverror_xgb) * ")")
),
name = element_blank()) +
scale_linetype_manual(
values = lty_vec,
labels = c("Actual",
bquote("BART (" * R^2 ~ "=" ~ .(rsq_bart) * "," ~ RMSE[cv] ~ "=" ~ .(cverror_bart) * ")"),
bquote("eNET (" * R^2 ~ "=" ~ .(rsq_lre) * "," ~ RMSE[cv] ~ "=" ~ .(cverror_lre) * ")"),
bquote("XGB (" * R^2 ~ "=" ~ .(rsq_xgb) * "," ~ RMSE[cv] ~ "=" ~ .(cverror_xgb) * ")")
),
name = element_blank()) +
scale_alpha_manual(
values = alpha_vec,
labels = c("Actual",
bquote("BART (" * R^2 ~ "=" ~ .(rsq_bart) * "," ~ RMSE[cv] ~ "=" ~ .(cverror_bart) * ")"),
bquote("eNET (" * R^2 ~ "=" ~ .(rsq_lre) * "," ~ RMSE[cv] ~ "=" ~ .(cverror_lre) * ")"),
bquote("XGB (" * R^2 ~ "=" ~ .(rsq_xgb) * "," ~ RMSE[cv] ~ "=" ~ .(cverror_xgb) * ")")
),
name = element_blank()) +
scale_y_continuous(labels = function(x) paste0(x)) +
xlab("Index (County x Event)") +
ylab("Hours (ln)") +
ggtitle("High-Wind Outage Duration: Test Sample") +
#ylab("Max Customers Out (ln - %)") +
#ggtitle("Hurricane Outages: Test Sample") +
# guides(
#   color = guide_legend(order = 2),
#   shape = guide_legend(order = 1),
#   linetype = guide_legend(order = 2)
# ) +
theme(legend.spacing.y = unit(-0.25, "cm"),
legend.direction = "vertical",
legend.box = "vertical",
legend.position = c(.225, .8),
plot.title = element_text(hjust = 0.5)
)
print(p)
}
plot_filtering_estimates2(gg)
final_lre = lre_work %>%
finalize_workflow(lre_best) %>%
fit(df_data)
print(tidy(final_lre), n = 150)
final_model = xgb_work %>%
finalize_workflow(xgb_best) %>%
fit(df_data)
final_obj = extract_fit_parsnip(final_model)$fit
importance = xgb.importance(model = final_obj)
head(importance, n = 20)
vip(final_obj, n = 20)
bart_vimp = investigate_var_importance(bart_fit)
save.image("~/vSandia2/Data/DeepDive/highwind_hrs.RData")
